{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca64afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.ops as ops\n",
    "import math\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "YT_ROOTS = [os.getenv(\"YT_ROOT_1\"), os.getenv(\"YT_ROOT_2\")]\n",
    "\n",
    "\n",
    "def extract_bbox_from_points(points):\n",
    "    xs = [p[0] for p in points]\n",
    "    ys = [p[1] for p in points]\n",
    "    x1, x2 = min(xs), max(xs)\n",
    "    y1, y2 = min(ys), max(ys)\n",
    "    return [x1, y1, x2 - x1, y2 - y1]  # x, y, w, h\n",
    "\n",
    "\n",
    "def get_brake_status(raw):\n",
    "    if \"BrakeOn\" in raw:\n",
    "        return \"brake_on\"\n",
    "    if \"BrakeOff\" in raw:\n",
    "        return \"brake_off\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def get_turn_signal(ts):\n",
    "    if ts == \"left\":\n",
    "        return \"left_signal\"\n",
    "    if ts == \"right\":\n",
    "        return \"right_signal\"\n",
    "    if ts == \"hazard\":\n",
    "        return \"hazard\"\n",
    "    if ts == \"off\":\n",
    "        return \"off\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def normalize_label(shape):\n",
    "    raw = shape[\"label\"]\n",
    "    ts = shape.get(\"attributes\", {}).get(\"turn_signal\", \"\")\n",
    "    return get_brake_status(raw), get_turn_signal(ts)\n",
    "\n",
    "\n",
    "def process_json(json_path):\n",
    "    img_path = json_path[:-5] + \".jpg\"\n",
    "    if not os.path.exists(img_path):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            ann = json.load(f)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    objects = []\n",
    "    for shape in ann.get(\"shapes\", []):\n",
    "        bbox = extract_bbox_from_points(shape[\"points\"])\n",
    "        brake_status, turn_signal = normalize_label(shape)\n",
    "        if brake_status != \"unknown\":\n",
    "            objects.append({\"bbox\": bbox, \"label\": brake_status})\n",
    "        if turn_signal not in [\"unknown\", \"off\"]:\n",
    "            objects.append({\"bbox\": bbox, \"label\": turn_signal})\n",
    "\n",
    "    return {\"image_path\": img_path, \"objects\": objects}\n",
    "\n",
    "\n",
    "def load_yt_dataset_fast(root_dirs, max_workers=16):\n",
    "    json_files = [\n",
    "        os.path.join(dirpath, f)\n",
    "        for root in root_dirs\n",
    "        for dirpath, _, filenames in os.walk(root)\n",
    "        for f in filenames\n",
    "        if f.lower().endswith(\".json\")\n",
    "    ]\n",
    "    print(\"finished finding all JSON files\")\n",
    "\n",
    "    samples = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_json, jf): jf for jf in json_files}\n",
    "        for idx, future in enumerate(as_completed(futures)):\n",
    "            res = future.result()\n",
    "            if res:\n",
    "                samples.append(res)\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"Processed {idx+1} / {len(json_files)} JSON files\")\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90782c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to load YT dataset...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mstarting to load YT dataset...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m samples = \u001b[43mload_yt_dataset_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mYT_ROOTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal samples:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(samples))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mload_yt_dataset_fast\u001b[39m\u001b[34m(root_dirs, max_workers)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_yt_dataset_fast\u001b[39m(root_dirs, max_workers=\u001b[32m16\u001b[39m):\n\u001b[32m     85\u001b[39m     json_files = \u001b[43m[\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot_dirs\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdirpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwalk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mfinished finding all JSON files\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     94\u001b[39m     samples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:351\u001b[39m, in \u001b[36mwalk\u001b[39m\u001b[34m(top, topdown, onerror, followlinks)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "print('starting to load YT dataset...')\n",
    "samples = load_yt_dataset_fast(YT_ROOTS)\n",
    "print(\"Total samples:\", len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fe4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cb4f3",
   "metadata": {},
   "source": [
    "Where does OWL-ViT fail in this safety-critical scenario?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"brake_off\": \"car with brake light off\",\n",
    "    \"brake_on\": \"car with brake light on\",\n",
    "    \"left_signal\": \"car with left signal on\",\n",
    "    \"right_signal\": \"car with right signal on\",\n",
    "    \"hazard\": \"car with hazard lights on\",\n",
    "}\n",
    "\n",
    "text_queries = list(label_map.values())\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "label_to_idx = {key: idx for idx, key in enumerate(label_map.keys())}\n",
    "\n",
    "\n",
    "class OwlViTDataset(Dataset):\n",
    "    def __init__(self, records, label_map, min_boxes=1):\n",
    "        self.label_map = label_map\n",
    "        self.records = [\n",
    "            record\n",
    "            for record in records\n",
    "            if sum(obj[\"label\"] in label_map for obj in record[\"objects\"]) >= min_boxes\n",
    "        ]\n",
    "        self.label_to_idx = label_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.records[idx]\n",
    "        img_path = item[\"image_path\"]\n",
    "        objects = item[\"objects\"]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "\n",
    "        boxes = []\n",
    "        labels_idx = []\n",
    "        for obj in objects:\n",
    "            class_name = obj[\"label\"]\n",
    "            if class_name not in self.label_map:\n",
    "                continue\n",
    "\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            x1 = max(x / width, 0.0)\n",
    "            y1 = max(y / height, 0.0)\n",
    "            x2 = min((x + w) / width, 1.0)\n",
    "            y2 = min((y + h) / height, 1.0)\n",
    "\n",
    "            boxes.append([float(x1), float(y1), float(x2), float(y2)])\n",
    "            labels_idx.append(self.label_to_idx[class_name])\n",
    "\n",
    "        if boxes:\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "        else:\n",
    "            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if labels_idx:\n",
    "            labels_tensor = torch.tensor(labels_idx, dtype=torch.long)\n",
    "        else:\n",
    "            labels_tensor = torch.zeros((0,), dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"boxes\": boxes_tensor,\n",
    "            \"labels\": labels_tensor,\n",
    "            \"image_path\": img_path,\n",
    "            \"size\": (height, width),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1a98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(records, train_ratio=0.85, seed=42):\n",
    "    indices = list(range(len(records)))\n",
    "    random.Random(seed).shuffle(indices)\n",
    "    split_idx = max(1, int(len(indices) * train_ratio))\n",
    "    train_records = [records[i] for i in indices[:split_idx]]\n",
    "    val_records = [records[i] for i in indices[split_idx:]]\n",
    "    if not val_records:\n",
    "        val_records = train_records[-1:]\n",
    "        train_records = train_records[:-1]\n",
    "    return train_records, val_records\n",
    "\n",
    "\n",
    "def owlvit_collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    batch_text = [text_queries] * len(images)\n",
    "    encoded = processor(images=images, text=batch_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": encoded[\"pixel_values\"],\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "        \"gt_boxes\": [item[\"boxes\"] for item in batch],\n",
    "        \"gt_labels\": [item[\"labels\"] for item in batch],\n",
    "        \"image_paths\": [item[\"image_path\"] for item in batch],\n",
    "    }\n",
    "\n",
    "\n",
    "def cxcywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return torch.stack([x1, y1, x2, y2], dim=-1)\n",
    "\n",
    "\n",
    "def xyxy_to_cxcywh(boxes):\n",
    "    x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    cx = x1 + 0.5 * w\n",
    "    cy = y1 + 0.5 * h\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def owlvit_detection_loss(outputs, gt_boxes, gt_labels, cls_weight=1.0, box_weight=2.0, iou_weight=1.0):\n",
    "    pred_boxes = outputs.pred_boxes\n",
    "    pred_logits = outputs.logits\n",
    "    device = pred_boxes.device\n",
    "    total_loss = torch.zeros(1, device=device)\n",
    "    matched_batches = 0\n",
    "\n",
    "    for batch_idx in range(len(gt_boxes)):\n",
    "        boxes = gt_boxes[batch_idx].to(device)\n",
    "        labels = gt_labels[batch_idx].to(device)\n",
    "        if boxes.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        preds_xyxy = cxcywh_to_xyxy(pred_boxes[batch_idx])\n",
    "        ious = ops.box_iou(preds_xyxy, boxes)\n",
    "        best_idx = torch.argmax(ious, dim=0)\n",
    "        matched_logits = pred_logits[batch_idx][best_idx]\n",
    "        cls_loss = F.cross_entropy(matched_logits, labels)\n",
    "\n",
    "        pred_cxcywh = pred_boxes[batch_idx][best_idx]\n",
    "        target_cxcywh = xyxy_to_cxcywh(boxes)\n",
    "        box_loss = F.l1_loss(pred_cxcywh, target_cxcywh)\n",
    "\n",
    "        matched_ious = ious[best_idx, torch.arange(len(boxes), device=device)]\n",
    "        iou_loss = (1.0 - matched_ious.clamp(0.0, 1.0)).mean()\n",
    "\n",
    "        total_loss = total_loss + cls_weight * cls_loss + box_weight * box_loss + iou_weight * iou_loss\n",
    "        matched_batches += 1\n",
    "\n",
    "    if matched_batches == 0:\n",
    "        return total_loss\n",
    "\n",
    "    return total_loss / matched_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e862888d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_records, val_records = train_val_split(\u001b[43msamples\u001b[49m, train_ratio=\u001b[32m0.70\u001b[39m, seed=\u001b[32m42\u001b[39m)\n\u001b[32m      2\u001b[39m train_dataset = OwlViTDataset(train_records, label_map)\n\u001b[32m      3\u001b[39m val_dataset = OwlViTDataset(val_records, label_map)\n",
      "\u001b[31mNameError\u001b[39m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "train_records, val_records = train_val_split(samples, train_ratio=0.70, seed=42)\n",
    "train_dataset = OwlViTDataset(train_records, label_map)\n",
    "val_dataset = OwlViTDataset(val_records, label_map)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bcbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 4\n",
    "learning_rate = 5e-6\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=owlvit_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=owlvit_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "total_train_steps = max(1, len(train_loader) * num_epochs)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd99ef98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b075c34f684e50a7738435a7c6d1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345a73892e014c07a783745e49440a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | train loss: 0.4499 | val loss: 0.3777\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe81b35673a1408a95c4af97a7b9f0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c839ff9d12d4e6ca1d3fae5c7b99b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 | train loss: 0.3202 | val loss: 0.3385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d702b281648e40fc8f66259319133d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db51ec6733b6478eac0335dd506f5249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 | train loss: 0.2547 | val loss: 0.3320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57dfe1bf09a46e0bf05935244f4cf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be65797a49dc4ed48869bf348cba5ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 | train loss: 0.2199 | val loss: 0.3324\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(data_loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    steps = 0\n",
    "    loop = tqdm(data_loader, desc=\"train\" if train else \"val\", leave=False)\n",
    "\n",
    "    for batch in loop:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            loss = owlvit_detection_loss(\n",
    "                outputs,\n",
    "                gt_boxes=batch[\"gt_boxes\"],\n",
    "                gt_labels=batch[\"gt_labels\"],\n",
    "            )\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        steps += 1\n",
    "        loop.set_postfix(loss=epoch_loss / max(1, steps))\n",
    "\n",
    "    return epoch_loss / max(1, steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned OWL-ViT artifacts to artifacts\\owlvit-finetune\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(\"artifacts\", f\"owlvit-finetune\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "processor.save_pretrained(save_dir)\n",
    "print(f\"Saved fine-tuned OWL-ViT artifacts to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d7deab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OwlViTForObjectDetection(\n",
       "  (owlvit): OwlViTModel(\n",
       "    (text_model): OwlViTTextTransformer(\n",
       "      (embeddings): OwlViTTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (position_embedding): Embedding(16, 512)\n",
       "      )\n",
       "      (encoder): OwlViTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x OwlViTEncoderLayer(\n",
       "            (self_attn): OwlViTAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): OwlViTMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_model): OwlViTVisionTransformer(\n",
       "      (embeddings): OwlViTVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "        (position_embedding): Embedding(577, 768)\n",
       "      )\n",
       "      (pre_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): OwlViTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x OwlViTEncoderLayer(\n",
       "            (self_attn): OwlViTAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): OwlViTMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (class_head): OwlViTClassPredictionHead(\n",
       "    (dense0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (logit_shift): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (logit_scale): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "  )\n",
       "  (box_head): OwlViTBoxPredictionHead(\n",
       "    (dense0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dense1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (dense2): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dir = \"artifacts/owlvit-finetune\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "finetuned_processor = OwlViTProcessor.from_pretrained(ckpt_dir)\n",
    "finetuned_model = OwlViTForObjectDetection.from_pretrained(ckpt_dir).to(device)\n",
    "finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d7ad1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_records' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m random_seed = \u001b[32m123\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Ensure deterministic subset selection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m random.Random(random_seed).shuffle(\u001b[43mval_records\u001b[49m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEval config: max_eval_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_eval_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m stride=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_stride\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iou_th=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miou_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m visualize=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisualize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'val_records' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluation configuration\n",
    "max_eval_samples = 300  # cap for speed; set None to use all validation samples\n",
    "sample_stride = 1       # take every Nth sample if dataset large\n",
    "iou_threshold = 0.5     # IoU threshold for a correct localization\n",
    "score_threshold = 0.20  # Probability threshold after softmax over label prompts (0-1)\n",
    "visualize = True        # toggle visualization output\n",
    "visualization_samples = 8  # number of random samples to visualize (<= max_eval_samples)\n",
    "random_seed = 123\n",
    "\n",
    "# Ensure deterministic subset selection\n",
    "random.Random(random_seed).shuffle(val_records)\n",
    "\n",
    "print(f\"Eval config: max_eval_samples={max_eval_samples} stride={sample_stride} iou_th={iou_threshold} prob_th={score_threshold} visualize={visualize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d114516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded evaluation helpers.\n"
     ]
    }
   ],
   "source": [
    "def box_iou_xyxy(a, b):\n",
    "    if a.numel() == 0 or b.numel() == 0:\n",
    "        return torch.zeros((a.shape[0], b.shape[0]))\n",
    "    inter_x1 = torch.max(a[:, None, 0], b[:, 0])\n",
    "    inter_y1 = torch.max(a[:, None, 1], b[:, 1])\n",
    "    inter_x2 = torch.min(a[:, None, 2], b[:, 2])\n",
    "    inter_y2 = torch.min(a[:, None, 3], b[:, 3])\n",
    "    inter_w = (inter_x2 - inter_x1).clamp(min=0)\n",
    "    inter_h = (inter_y2 - inter_y1).clamp(min=0)\n",
    "    inter = inter_w * inter_h\n",
    "    area_a = (a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1])\n",
    "    area_b = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "    union = area_a[:, None] + area_b - inter\n",
    "    return inter / union.clamp(min=1e-6)\n",
    "\n",
    "\n",
    "def match_predictions_to_gt(pred_boxes, pred_scores, gt_boxes, gt_labels, iou_th=0.5, score_th=0.0):\n",
    "    if pred_boxes.numel() == 0:\n",
    "        return 0, 0, 0\n",
    "    keep = pred_scores >= score_th\n",
    "    if keep.sum() == 0:\n",
    "        return 0, 0, gt_boxes.shape[0]\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    pred_scores = pred_scores[keep]\n",
    "\n",
    "    cx, cy, w, h = pred_boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    pred_xyxy = torch.stack([x1, y1, x2, y2], dim=-1)\n",
    "    if gt_boxes.numel() == 0:\n",
    "        return 0, pred_boxes.shape[0], 0\n",
    "\n",
    "    ious = box_iou_xyxy(pred_xyxy, gt_boxes)\n",
    "    matched_gt = set()\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for pred_idx in torch.argsort(pred_scores, descending=True):\n",
    "        best_gt = torch.argmax(ious[pred_idx])\n",
    "        best_iou = ious[pred_idx, best_gt].item()\n",
    "        if best_iou >= iou_th and best_gt.item() not in matched_gt:\n",
    "            matched_gt.add(best_gt.item())\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    fn = gt_boxes.shape[0] - len(matched_gt)\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def evaluate_models(records, pretrained_model, finetuned_model, processor, label_map, max_samples=None, stride=1, iou_th=0.5, score_th=0.0):\n",
    "    label_texts = list(label_map.values())\n",
    "    subset = records[::stride]\n",
    "    if max_samples is not None:\n",
    "        subset = subset[:max_samples]\n",
    "\n",
    "    metrics = {\n",
    "        'pretrained': {'tp':0,'fp':0,'fn':0},\n",
    "        'finetuned': {'tp':0,'fp':0,'fn':0},\n",
    "    }\n",
    "\n",
    "    for r in tqdm(subset, desc='eval', leave=False):\n",
    "        image = Image.open(r['image_path']).convert('RGB')\n",
    "        gt_boxes_list = []\n",
    "        gt_labels_list = []\n",
    "        for obj in r['objects']:\n",
    "            if obj['label'] in label_map:\n",
    "                x, y, w, h = obj['bbox']\n",
    "                width, height = image.size\n",
    "                x1 = x / width\n",
    "                y1 = y / height\n",
    "                x2 = (x + w) / width\n",
    "                y2 = (y + h) / height\n",
    "                gt_boxes_list.append([x1, y1, x2, y2])\n",
    "                gt_labels_list.append(label_to_idx[obj['label']])\n",
    "        if gt_boxes_list:\n",
    "            gt_boxes_tensor = torch.tensor(gt_boxes_list, dtype=torch.float32)\n",
    "        else:\n",
    "            gt_boxes_tensor = torch.zeros((0,4), dtype=torch.float32)\n",
    "        if gt_labels_list:\n",
    "            gt_labels_tensor = torch.tensor(gt_labels_list, dtype=torch.long)\n",
    "        else:\n",
    "            gt_labels_tensor = torch.zeros((0,), dtype=torch.long)\n",
    "\n",
    "        enc = processor(images=[image], text=[label_texts], return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            pt_out = pretrained_model(**{k: v.to(device) for k,v in enc.items() if k in ['pixel_values','input_ids','attention_mask']})\n",
    "            ft_out = finetuned_model(**{k: v.to(device) for k,v in enc.items() if k in ['pixel_values','input_ids','attention_mask']})\n",
    "\n",
    "        def extract_probs(outputs):\n",
    "            logits = outputs.logits[0]\n",
    "            if logits.shape[-1] != len(label_texts):\n",
    "                sz = min(logits.shape[-1], len(label_texts))\n",
    "                logits = logits[:, :sz]\n",
    "            probs = torch.softmax(logits, dim=-1)  # [num_queries, num_labels]\n",
    "            max_probs, pred_label_idx = torch.max(probs, dim=-1)\n",
    "            return max_probs\n",
    "        pt_probs = extract_probs(pt_out).cpu()\n",
    "        ft_probs = extract_probs(ft_out).cpu()\n",
    "        pred_boxes_pt = pt_out.pred_boxes[0].cpu()\n",
    "        pred_boxes_ft = ft_out.pred_boxes[0].cpu()\n",
    "\n",
    "        tp, fp, fn = match_predictions_to_gt(pred_boxes_pt, pt_probs, gt_boxes_tensor, gt_labels_tensor, iou_th=iou_th, score_th=score_th)\n",
    "        metrics['pretrained']['tp'] += tp\n",
    "        metrics['pretrained']['fp'] += fp\n",
    "        metrics['pretrained']['fn'] += fn\n",
    "\n",
    "        tp, fp, fn = match_predictions_to_gt(pred_boxes_ft, ft_probs, gt_boxes_tensor, gt_labels_tensor, iou_th=iou_th, score_th=score_th)\n",
    "        metrics['finetuned']['tp'] += tp\n",
    "        metrics['finetuned']['fp'] += fp\n",
    "        metrics['finetuned']['fn'] += fn\n",
    "\n",
    "    def finalize(m):\n",
    "        precision = m['tp'] / max(1, (m['tp'] + m['fp']))\n",
    "        recall = m['tp'] / max(1, (m['tp'] + m['fn']))\n",
    "        f1 = 2*precision*recall / max(1e-6, (precision + recall))\n",
    "        return {**m, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "    return {k: finalize(v) for k,v in metrics.items()}\n",
    "\n",
    "\n",
    "def visualize_examples(records, finetuned_model, pretrained_model, processor, label_map, num_samples=6):\n",
    "    samples_vis = random.sample(records, min(num_samples, len(records)))\n",
    "    label_texts = list(label_map.values())\n",
    "    for r in samples_vis:\n",
    "        image = Image.open(r['image_path']).convert('RGB')\n",
    "        width, height = image.size\n",
    "        fig, axes = plt.subplots(1,2, figsize=(12,6))\n",
    "        for ax, mdl, title in zip(axes, [pretrained_model, finetuned_model], ['Pretrained','Finetuned']):\n",
    "            enc = processor(images=[image], text=[label_texts], return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                out = mdl(**{k: v.to(device) for k,v in enc.items() if k in ['pixel_values','input_ids','attention_mask']})\n",
    "            logits = out.logits[0]\n",
    "            if logits.shape[-1] != len(label_texts):\n",
    "                sz = min(logits.shape[-1], len(label_texts))\n",
    "                logits = logits[:, :sz]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            max_probs, _ = torch.max(probs, dim=-1)\n",
    "            keep = max_probs.cpu() >= score_threshold\n",
    "            pred_boxes = out.pred_boxes[0].cpu()[keep]\n",
    "            cx, cy, w, h = pred_boxes.unbind(-1)\n",
    "            x1 = (cx - 0.5*w) * width\n",
    "            y1 = (cy - 0.5*h) * height\n",
    "            x2 = (cx + 0.5*w) * width\n",
    "            y2 = (cy + 0.5*h) * height\n",
    "            ax.imshow(image)\n",
    "            for bx1, by1, bx2, by2, prob in zip(x1, y1, x2, y2, max_probs[keep]):\n",
    "                rect = patches.Rectangle((bx1.item(), by1.item()), (bx2-bx1).item(), (by2-by1).item(), linewidth=1, edgecolor='cyan', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(bx1.item(), by1.item()-5, f\"{prob.item():.2f}\", color='cyan', fontsize=8)\n",
    "            for obj in r['objects']:\n",
    "                if obj['label'] in label_map:\n",
    "                    gx, gy, gw, gh = obj['bbox']\n",
    "                    rect = patches.Rectangle((gx, gy), gw, gh, linewidth=2, edgecolor='red', facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "            ax.set_title(f\"{title} (prob>={score_threshold})\")\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Loaded evaluation helpers with probability thresholding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574916a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_records' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m records_for_eval = \u001b[43mval_records\u001b[49m\n\u001b[32m      3\u001b[39m metrics = evaluate_models(records_for_eval, model, finetuned_model, processor, label_map,\n\u001b[32m      4\u001b[39m                           max_samples=max_eval_samples, stride=sample_stride, iou_th=iou_threshold)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluation metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'val_records' is not defined"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "records_for_eval = val_records\n",
    "metrics = evaluate_models(records_for_eval, model, finetuned_model, processor, label_map,\n",
    "                          max_samples=max_eval_samples, stride=sample_stride, iou_th=iou_threshold, score_th=score_threshold)\n",
    "print(\"Evaluation metrics (IoU>=%.2f, prob>=%.2f):\" % (iou_threshold, score_threshold))\n",
    "for k,v in metrics.items():\n",
    "    print(f\"{k}: tp={v['tp']} fp={v['fp']} fn={v['fn']} precision={v['precision']:.3f} recall={v['recall']:.3f} f1={v['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc91f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 8 examples...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_records' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVisualizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisualization_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m examples...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     visualize_examples(\u001b[43mval_records\u001b[49m[:max_eval_samples \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_records)], finetuned_model, model, processor, label_map, num_samples=visualization_samples)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVisualization disabled. Set visualize=True to enable.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'val_records' is not defined"
     ]
    }
   ],
   "source": [
    "if visualize:\n",
    "    print(f\"Visualizing {visualization_samples} examples...\")\n",
    "    visualize_examples(val_records[:max_eval_samples or len(val_records)], finetuned_model, model, processor, label_map, num_samples=visualization_samples)\n",
    "else:\n",
    "    print(\"Visualization disabled. Set visualize=True to enable.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
