{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca64afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# print(sys.executable)\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torchvision.ops as ops\n",
    "import math\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "YT_ROOTS = [\n",
    "    os.getenv('YT_ROOT_1'),\n",
    "    os.getenv('YT_ROOT_2')\n",
    "]\n",
    "\n",
    "def extract_bbox_from_points(points):\n",
    "    xs = [p[0] for p in points]\n",
    "    ys = [p[1] for p in points]\n",
    "    x1, x2 = min(xs), max(xs)\n",
    "    y1, y2 = min(ys), max(ys)\n",
    "    return [x1, y1, x2 - x1, y2 - y1]  # x, y, w, h\n",
    "# brake_on, brake_off, left_signal, right_signal, hazard, off\n",
    "def normalize_label(shape):\n",
    "    raw = shape[\"label\"]\n",
    "\n",
    "    if \"BrakeOn\" in raw:\n",
    "        return \"brake_on\"\n",
    "    if \"BrakeOff\" in raw:\n",
    "        return \"brake_off\"\n",
    "\n",
    "    ts = shape.get(\"attributes\", {}).get(\"turn_signal\", \"\")\n",
    "    if ts == \"left\":\n",
    "        return \"left_signal\"\n",
    "    if ts == \"right\":\n",
    "        return \"right_signal\"\n",
    "    if ts == \"hazard\":\n",
    "        return \"hazard\"\n",
    "    if ts == \"off\":\n",
    "        return \"off\"\n",
    "    print(ts)\n",
    "    return \"unknown\"\n",
    "\n",
    "def load_yt_dataset(root_dirs):\n",
    "    samples = []\n",
    "\n",
    "    for root in root_dirs:\n",
    "        for dirpath, _, filenames in os.walk(root):\n",
    "            jpg_files = [f for f in filenames if f.lower().endswith(\".jpg\")]\n",
    "\n",
    "            for jpg in jpg_files:\n",
    "                img_path = os.path.join(dirpath, jpg)\n",
    "                json_path = os.path.join(dirpath, jpg.replace(\".jpg\", \".json\"))\n",
    "\n",
    "                if not os.path.exists(json_path):\n",
    "                    # skip images without annotations\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(json_path, \"r\") as f:\n",
    "                        ann = json.load(f)\n",
    "                except:\n",
    "                    # skip corrupted jsons\n",
    "                    continue\n",
    "\n",
    "                objects = []\n",
    "                for shape in ann.get(\"shapes\", []):\n",
    "                    bbox = extract_bbox_from_points(shape[\"points\"])\n",
    "                    label = normalize_label(shape)\n",
    "                    objects.append({\"bbox\": bbox, \"label\": label})\n",
    "\n",
    "                samples.append({\n",
    "                    \"image_path\": img_path,\n",
    "                    \"objects\": objects\n",
    "                })\n",
    "                if (len(samples) % 1000 == 0):\n",
    "                    print(f\"Loaded {len(samples)} samples out of 111800\")\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90782c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to load YT dataset...\n",
      "Loaded 1000 samples out of 111800\n",
      "Loaded 2000 samples out of 111800\n",
      "Loaded 3000 samples out of 111800\n",
      "Loaded 4000 samples out of 111800\n",
      "Loaded 5000 samples out of 111800\n",
      "Loaded 6000 samples out of 111800\n",
      "Loaded 7000 samples out of 111800\n",
      "Loaded 8000 samples out of 111800\n",
      "Loaded 9000 samples out of 111800\n",
      "Loaded 10000 samples out of 111800\n",
      "Loaded 11000 samples out of 111800\n",
      "Loaded 12000 samples out of 111800\n",
      "Loaded 13000 samples out of 111800\n",
      "Loaded 14000 samples out of 111800\n",
      "Loaded 15000 samples out of 111800\n",
      "Loaded 16000 samples out of 111800\n",
      "Loaded 17000 samples out of 111800\n",
      "Loaded 18000 samples out of 111800\n",
      "Loaded 19000 samples out of 111800\n",
      "Loaded 20000 samples out of 111800\n",
      "Loaded 21000 samples out of 111800\n",
      "Loaded 22000 samples out of 111800\n",
      "Loaded 23000 samples out of 111800\n",
      "Loaded 24000 samples out of 111800\n",
      "Loaded 25000 samples out of 111800\n",
      "Loaded 26000 samples out of 111800\n",
      "Loaded 27000 samples out of 111800\n",
      "Loaded 28000 samples out of 111800\n",
      "Loaded 29000 samples out of 111800\n",
      "Loaded 30000 samples out of 111800\n",
      "Loaded 31000 samples out of 111800\n",
      "Loaded 32000 samples out of 111800\n",
      "Loaded 33000 samples out of 111800\n",
      "Loaded 34000 samples out of 111800\n",
      "Loaded 35000 samples out of 111800\n",
      "Loaded 36000 samples out of 111800\n",
      "Loaded 37000 samples out of 111800\n",
      "Loaded 38000 samples out of 111800\n",
      "Loaded 39000 samples out of 111800\n",
      "Loaded 40000 samples out of 111800\n",
      "Loaded 41000 samples out of 111800\n",
      "Loaded 42000 samples out of 111800\n",
      "Loaded 43000 samples out of 111800\n",
      "Loaded 44000 samples out of 111800\n",
      "Loaded 45000 samples out of 111800\n"
     ]
    }
   ],
   "source": [
    "print('starting to load YT dataset...')\n",
    "samples = load_yt_dataset(YT_ROOTS)\n",
    "print(\"Total samples:\", len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fe4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "text_queries = [\n",
    "    \"vehicle\",\n",
    "    \"car tail light\",\n",
    "    \"brake light\",\n",
    "    \"left turn signal\",\n",
    "    \"right turn signal\",\n",
    "    \"car hazard lights\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(img_path, ann_objects, draw=False):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    width, height = img.size\n",
    "\n",
    "    inputs = processor(text=text_queries, images=img, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([(height, width)])\n",
    "    result = processor.post_process_grounded_object_detection(\n",
    "        outputs=outputs,\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=0.1,\n",
    "        text_labels=[text_queries],\n",
    "    )[0]\n",
    "\n",
    "    boxes, scores, labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n",
    "    if draw:\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 7))\n",
    "        ax.imshow(img)\n",
    "\n",
    "        # truth\n",
    "        for obj in ann_objects:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            ax.add_patch(patches.Rectangle(\n",
    "                (x, y), w, h, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "            ))\n",
    "            ax.text(x, y - 5, obj[\"label\"], color=\"red\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "        # Predictions\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            ax.add_patch(patches.Rectangle(\n",
    "                (x1, y1), w, h, linewidth=1.2,\n",
    "                edgecolor=\"lime\", facecolor=\"none\", zorder=10\n",
    "            ))\n",
    "            ax.text(\n",
    "                x1, y1 - 5, f\"{label} ({score:.2f})\",\n",
    "                color=\"lime\", fontsize=11, weight=\"bold\", zorder=11\n",
    "            )\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    loss_value = taillight_loss(\n",
    "        pred_boxes=torch.tensor(boxes),\n",
    "        pred_labels=labels,\n",
    "        pred_scores=scores,\n",
    "        ann_objects=ann_objects\n",
    "    )\n",
    "    if draw:\n",
    "        print(\"Loss:\", loss_value)\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def taillight_loss(pred_boxes, pred_labels, pred_scores, ann_objects):\n",
    "    \"\"\"\n",
    "    Computes a unified loss for taillight state detection.\n",
    "    Lower = better.\n",
    "    \n",
    "    Components:\n",
    "        - IoU loss: (1 - IoU)\n",
    "        - Class loss: 0 if correct, weight if wrong\n",
    "        - Confidence loss: (1 - score)\n",
    "        - Brake mistakes weighted heavier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Map model label â†’ canonical GT label\n",
    "    label_map = {\n",
    "        \"vehicle\": \"off\",\n",
    "        \"car tail light\": \"off\",\n",
    "        \"brake light\": \"brake_on\",\n",
    "        \"left turn signal\": \"left_signal\",\n",
    "        \"right turn signal\": \"right_signal\",\n",
    "        \"car hazard lights\": \"hazard\",\n",
    "    }\n",
    "\n",
    "    # Classification penalty weights (mistakes in brake_on cost more)\n",
    "    class_penalty = {\n",
    "        \"brake_on\": 4.0,       # biggest penalty\n",
    "        \"left_signal\": 2.0,\n",
    "        \"right_signal\": 2.0,\n",
    "        \"hazard\": 2.5,\n",
    "        \"off\": 1.0,\n",
    "        \"brake_off\": 1.0,\n",
    "    }\n",
    "    if len(ann_objects) == 0:\n",
    "        return 10.0 * len(pred_boxes) / math.log(len(pred_boxes) + 2)  # no objects = no loss\n",
    "\n",
    "    if pred_boxes is None or len(pred_boxes) == 0:\n",
    "        # Penalize missing all predictions\n",
    "        return 10.0 * len(ann_objects) / math.log(len(ann_objects) + 1)\n",
    "\n",
    "    # Convert GT boxes\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for obj in ann_objects:\n",
    "        x, y, w, h = obj[\"bbox\"]\n",
    "        gt_boxes.append([x, y, x + w, y + h])\n",
    "        gt_labels.append(obj[\"label\"])\n",
    "\n",
    "    gt_boxes = torch.tensor(gt_boxes).float()\n",
    "\n",
    "    # IoU matrix\n",
    "    iou_mat = ops.box_iou(pred_boxes, gt_boxes)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # For each GT object, match the best prediction\n",
    "    for gi, gt_label in enumerate(gt_labels):\n",
    "        best_pi = torch.argmax(iou_mat[:, gi])\n",
    "        best_iou = iou_mat[best_pi, gi].item()\n",
    "\n",
    "        pred_label_raw = pred_labels[best_pi]\n",
    "        pred_label = label_map.get(pred_label_raw, \"off\")\n",
    "        pred_conf = pred_scores[best_pi].item()\n",
    "\n",
    "        # --- Loss components ---\n",
    "        iou_loss = 1 - best_iou                   # 0 when perfect overlap\n",
    "        conf_loss = 1 - pred_conf                # 0 when high confidence\n",
    "\n",
    "        if pred_label != gt_label:\n",
    "            class_loss = class_penalty.get(gt_label, 1.0)\n",
    "        else:\n",
    "            class_loss = 0.0\n",
    "\n",
    "        # Total loss per object\n",
    "        obj_loss = iou_loss + conf_loss + class_loss\n",
    "        total_loss += obj_loss\n",
    "        # if draw:\n",
    "            # print(total_loss / math.log(len(ann_objects) + 1))\n",
    "    return total_loss / math.log(len(ann_objects) + 1)  # average over GT objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6b0c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18438\\AppData\\Local\\Temp\\ipykernel_17952\\3557804176.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred_boxes=torch.tensor(boxes),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.705260015984056\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "#     sample = samples[random.randint(0, len(samples) - 1)]\n",
    "#     print(f\"Showing sample {i}: {sample['image_path']}\")\n",
    "#     visualize_prediction(sample[\"image_path\"], sample[\"objects\"], draw=True)\n",
    "\n",
    "loss = 0.0\n",
    "for i in range(100):\n",
    "    sample = samples[random.randint(0, len(samples) - 1)]\n",
    "    loss += visualize_prediction(sample[\"image_path\"], sample[\"objects\"], draw=False)\n",
    "print(loss / 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cb4f3",
   "metadata": {},
   "source": [
    "Where does OWL-ViT fail in this safety-critical scenario?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c7d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
