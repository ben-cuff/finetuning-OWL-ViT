{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca64afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.ops as ops\n",
    "import math\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "YT_ROOTS = [os.getenv(\"YT_ROOT_1\"), os.getenv(\"YT_ROOT_2\")]\n",
    "\n",
    "\n",
    "def extract_bbox_from_points(points):\n",
    "    xs = [p[0] for p in points]\n",
    "    ys = [p[1] for p in points]\n",
    "    x1, x2 = min(xs), max(xs)\n",
    "    y1, y2 = min(ys), max(ys)\n",
    "    return [x1, y1, x2 - x1, y2 - y1]  # x, y, w, h\n",
    "\n",
    "\n",
    "def get_brake_status(raw):\n",
    "    if \"BrakeOn\" in raw:\n",
    "        return \"brake_on\"\n",
    "    if \"BrakeOff\" in raw:\n",
    "        return \"brake_off\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def get_turn_signal(ts):\n",
    "    if ts == \"left\":\n",
    "        return \"left_signal\"\n",
    "    if ts == \"right\":\n",
    "        return \"right_signal\"\n",
    "    if ts == \"hazard\":\n",
    "        return \"hazard\"\n",
    "    if ts == \"off\":\n",
    "        return \"off\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def normalize_label(shape):\n",
    "    raw = shape[\"label\"]\n",
    "    ts = shape.get(\"attributes\", {}).get(\"turn_signal\", \"\")\n",
    "    return get_brake_status(raw), get_turn_signal(ts)\n",
    "\n",
    "\n",
    "def process_json(json_path):\n",
    "    img_path = json_path[:-5] + \".jpg\"\n",
    "    if not os.path.exists(img_path):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            ann = json.load(f)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    objects = []\n",
    "    for shape in ann.get(\"shapes\", []):\n",
    "        bbox = extract_bbox_from_points(shape[\"points\"])\n",
    "        brake_status, turn_signal = normalize_label(shape)\n",
    "        if brake_status != \"unknown\":\n",
    "            objects.append({\"bbox\": bbox, \"label\": brake_status})\n",
    "        if turn_signal not in [\"unknown\", \"off\"]:\n",
    "            objects.append({\"bbox\": bbox, \"label\": turn_signal})\n",
    "\n",
    "    return {\"image_path\": img_path, \"objects\": objects}\n",
    "\n",
    "\n",
    "def load_yt_dataset_fast(root_dirs, max_workers=16):\n",
    "    json_files = [\n",
    "        os.path.join(dirpath, f)\n",
    "        for root in root_dirs\n",
    "        for dirpath, _, filenames in os.walk(root)\n",
    "        for f in filenames\n",
    "        if f.lower().endswith(\".json\")\n",
    "    ]\n",
    "    print(\"finished finding all JSON files\")\n",
    "\n",
    "    samples = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_json, jf): jf for jf in json_files}\n",
    "        for idx, future in enumerate(as_completed(futures)):\n",
    "            res = future.result()\n",
    "            if res:\n",
    "                samples.append(res)\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"Processed {idx+1} / {len(json_files)} JSON files\")\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90782c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to load YT dataset...\n",
      "finished finding all JSON files\n",
      "finished finding all JSON files\n",
      "Processed 1000 / 111800 JSON files\n",
      "Processed 2000 / 111800 JSON files\n",
      "Processed 1000 / 111800 JSON files\n",
      "Processed 2000 / 111800 JSON files\n",
      "Processed 3000 / 111800 JSON files\n",
      "Processed 3000 / 111800 JSON files\n",
      "Processed 4000 / 111800 JSON files\n",
      "Processed 4000 / 111800 JSON files\n",
      "Processed 5000 / 111800 JSON files\n",
      "Processed 5000 / 111800 JSON files\n",
      "Processed 6000 / 111800 JSON files\n",
      "Processed 7000 / 111800 JSON files\n",
      "Processed 6000 / 111800 JSON files\n",
      "Processed 7000 / 111800 JSON files\n",
      "Processed 8000 / 111800 JSON files\n",
      "Processed 9000 / 111800 JSON files\n",
      "Processed 8000 / 111800 JSON files\n",
      "Processed 9000 / 111800 JSON files\n",
      "Processed 10000 / 111800 JSON files\n",
      "Processed 10000 / 111800 JSON files\n",
      "Processed 11000 / 111800 JSON files\n",
      "Processed 11000 / 111800 JSON files\n",
      "Processed 12000 / 111800 JSON files\n",
      "Processed 12000 / 111800 JSON files\n",
      "Processed 13000 / 111800 JSON files\n",
      "Processed 13000 / 111800 JSON files\n",
      "Processed 14000 / 111800 JSON files\n",
      "Processed 14000 / 111800 JSON files\n",
      "Processed 15000 / 111800 JSON files\n",
      "Processed 15000 / 111800 JSON files\n",
      "Processed 16000 / 111800 JSON files\n",
      "Processed 16000 / 111800 JSON files\n",
      "Processed 17000 / 111800 JSON files\n",
      "Processed 17000 / 111800 JSON files\n",
      "Processed 18000 / 111800 JSON files\n",
      "Processed 18000 / 111800 JSON files\n",
      "Processed 19000 / 111800 JSON files\n",
      "Processed 19000 / 111800 JSON files\n",
      "Processed 20000 / 111800 JSON files\n",
      "Processed 20000 / 111800 JSON files\n",
      "Processed 21000 / 111800 JSON files\n",
      "Processed 21000 / 111800 JSON files\n",
      "Processed 22000 / 111800 JSON files\n",
      "Processed 22000 / 111800 JSON files\n",
      "Processed 23000 / 111800 JSON files\n",
      "Processed 23000 / 111800 JSON files\n",
      "Processed 24000 / 111800 JSON files\n",
      "Processed 24000 / 111800 JSON files\n",
      "Processed 25000 / 111800 JSON files\n",
      "Processed 25000 / 111800 JSON files\n",
      "Processed 26000 / 111800 JSON files\n",
      "Processed 26000 / 111800 JSON files\n",
      "Processed 27000 / 111800 JSON files\n",
      "Processed 27000 / 111800 JSON files\n",
      "Processed 28000 / 111800 JSON files\n",
      "Processed 28000 / 111800 JSON files\n",
      "Processed 29000 / 111800 JSON files\n",
      "Processed 29000 / 111800 JSON files\n",
      "Processed 30000 / 111800 JSON files\n",
      "Processed 30000 / 111800 JSON files\n",
      "Processed 31000 / 111800 JSON files\n",
      "Processed 31000 / 111800 JSON files\n",
      "Processed 32000 / 111800 JSON files\n",
      "Processed 32000 / 111800 JSON files\n",
      "Processed 33000 / 111800 JSON files\n",
      "Processed 33000 / 111800 JSON files\n",
      "Processed 34000 / 111800 JSON files\n",
      "Processed 34000 / 111800 JSON files\n",
      "Processed 35000 / 111800 JSON files\n",
      "Processed 35000 / 111800 JSON files\n",
      "Processed 36000 / 111800 JSON files\n",
      "Processed 36000 / 111800 JSON files\n",
      "Processed 37000 / 111800 JSON files\n",
      "Processed 37000 / 111800 JSON files\n",
      "Processed 38000 / 111800 JSON files\n",
      "Processed 38000 / 111800 JSON files\n",
      "Processed 39000 / 111800 JSON files\n",
      "Processed 39000 / 111800 JSON files\n",
      "Processed 40000 / 111800 JSON files\n",
      "Processed 40000 / 111800 JSON files\n",
      "Processed 41000 / 111800 JSON files\n",
      "Processed 41000 / 111800 JSON files\n",
      "Processed 42000 / 111800 JSON files\n",
      "Processed 42000 / 111800 JSON files\n",
      "Processed 43000 / 111800 JSON files\n",
      "Processed 43000 / 111800 JSON files\n",
      "Processed 44000 / 111800 JSON files\n",
      "Processed 44000 / 111800 JSON files\n",
      "Processed 45000 / 111800 JSON files\n",
      "Processed 45000 / 111800 JSON files\n",
      "Processed 46000 / 111800 JSON files\n",
      "Processed 46000 / 111800 JSON files\n",
      "Processed 47000 / 111800 JSON files\n",
      "Processed 47000 / 111800 JSON files\n",
      "Processed 48000 / 111800 JSON files\n",
      "Processed 48000 / 111800 JSON files\n",
      "Processed 49000 / 111800 JSON files\n",
      "Processed 49000 / 111800 JSON files\n",
      "Processed 50000 / 111800 JSON files\n",
      "Processed 50000 / 111800 JSON files\n",
      "Processed 51000 / 111800 JSON files\n",
      "Processed 51000 / 111800 JSON files\n",
      "Processed 52000 / 111800 JSON files\n",
      "Processed 52000 / 111800 JSON files\n",
      "Processed 53000 / 111800 JSON files\n",
      "Processed 53000 / 111800 JSON files\n",
      "Processed 54000 / 111800 JSON files\n",
      "Processed 54000 / 111800 JSON files\n",
      "Processed 55000 / 111800 JSON files\n",
      "Processed 55000 / 111800 JSON files\n",
      "Processed 56000 / 111800 JSON files\n",
      "Processed 56000 / 111800 JSON files\n",
      "Processed 57000 / 111800 JSON files\n",
      "Processed 57000 / 111800 JSON files\n",
      "Processed 58000 / 111800 JSON files\n",
      "Processed 58000 / 111800 JSON files\n",
      "Processed 59000 / 111800 JSON files\n",
      "Processed 59000 / 111800 JSON files\n",
      "Processed 60000 / 111800 JSON files\n",
      "Processed 60000 / 111800 JSON files\n",
      "Processed 61000 / 111800 JSON files\n",
      "Processed 61000 / 111800 JSON files\n",
      "Processed 62000 / 111800 JSON files\n",
      "Processed 62000 / 111800 JSON files\n",
      "Processed 63000 / 111800 JSON files\n",
      "Processed 63000 / 111800 JSON files\n",
      "Processed 64000 / 111800 JSON files\n",
      "Processed 64000 / 111800 JSON files\n",
      "Processed 65000 / 111800 JSON files\n",
      "Processed 65000 / 111800 JSON files\n",
      "Processed 66000 / 111800 JSON files\n",
      "Processed 66000 / 111800 JSON files\n",
      "Processed 67000 / 111800 JSON files\n",
      "Processed 67000 / 111800 JSON files\n",
      "Processed 68000 / 111800 JSON files\n",
      "Processed 68000 / 111800 JSON files\n",
      "Processed 69000 / 111800 JSON files\n",
      "Processed 69000 / 111800 JSON files\n",
      "Processed 70000 / 111800 JSON files\n",
      "Processed 70000 / 111800 JSON files\n",
      "Processed 71000 / 111800 JSON files\n",
      "Processed 71000 / 111800 JSON files\n",
      "Processed 72000 / 111800 JSON files\n",
      "Processed 72000 / 111800 JSON files\n",
      "Processed 73000 / 111800 JSON files\n",
      "Processed 73000 / 111800 JSON files\n",
      "Processed 74000 / 111800 JSON files\n",
      "Processed 74000 / 111800 JSON files\n",
      "Processed 75000 / 111800 JSON files\n",
      "Processed 75000 / 111800 JSON files\n",
      "Processed 76000 / 111800 JSON files\n",
      "Processed 76000 / 111800 JSON files\n",
      "Processed 77000 / 111800 JSON files\n",
      "Processed 77000 / 111800 JSON files\n",
      "Processed 78000 / 111800 JSON files\n",
      "Processed 78000 / 111800 JSON files\n",
      "Processed 79000 / 111800 JSON files\n",
      "Processed 79000 / 111800 JSON files\n",
      "Processed 80000 / 111800 JSON files\n",
      "Processed 80000 / 111800 JSON files\n",
      "Processed 81000 / 111800 JSON files\n",
      "Processed 81000 / 111800 JSON files\n",
      "Processed 82000 / 111800 JSON files\n",
      "Processed 82000 / 111800 JSON files\n",
      "Processed 83000 / 111800 JSON files\n",
      "Processed 83000 / 111800 JSON files\n",
      "Processed 84000 / 111800 JSON files\n",
      "Processed 84000 / 111800 JSON files\n",
      "Processed 85000 / 111800 JSON files\n",
      "Processed 85000 / 111800 JSON files\n",
      "Processed 86000 / 111800 JSON files\n",
      "Processed 86000 / 111800 JSON files\n",
      "Processed 87000 / 111800 JSON files\n",
      "Processed 87000 / 111800 JSON files\n",
      "Processed 88000 / 111800 JSON files\n",
      "Processed 88000 / 111800 JSON files\n",
      "Processed 89000 / 111800 JSON files\n",
      "Processed 89000 / 111800 JSON files\n",
      "Processed 90000 / 111800 JSON files\n",
      "Processed 90000 / 111800 JSON files\n",
      "Processed 91000 / 111800 JSON files\n",
      "Processed 91000 / 111800 JSON files\n",
      "Processed 92000 / 111800 JSON files\n",
      "Processed 92000 / 111800 JSON files\n",
      "Processed 93000 / 111800 JSON files\n",
      "Processed 93000 / 111800 JSON files\n",
      "Processed 94000 / 111800 JSON files\n",
      "Processed 94000 / 111800 JSON files\n",
      "Processed 95000 / 111800 JSON files\n",
      "Processed 95000 / 111800 JSON files\n",
      "Processed 96000 / 111800 JSON files\n",
      "Processed 96000 / 111800 JSON files\n",
      "Processed 97000 / 111800 JSON files\n",
      "Processed 97000 / 111800 JSON files\n",
      "Processed 98000 / 111800 JSON files\n",
      "Processed 98000 / 111800 JSON files\n",
      "Processed 99000 / 111800 JSON files\n",
      "Processed 99000 / 111800 JSON files\n",
      "Processed 100000 / 111800 JSON files\n",
      "Processed 100000 / 111800 JSON files\n",
      "Processed 101000 / 111800 JSON files\n",
      "Processed 101000 / 111800 JSON files\n",
      "Processed 102000 / 111800 JSON files\n",
      "Processed 102000 / 111800 JSON files\n",
      "Processed 103000 / 111800 JSON files\n",
      "Processed 103000 / 111800 JSON files\n",
      "Processed 104000 / 111800 JSON files\n",
      "Processed 104000 / 111800 JSON files\n",
      "Processed 105000 / 111800 JSON files\n",
      "Processed 105000 / 111800 JSON files\n",
      "Processed 106000 / 111800 JSON files\n",
      "Processed 106000 / 111800 JSON files\n",
      "Processed 107000 / 111800 JSON files\n",
      "Processed 107000 / 111800 JSON files\n",
      "Processed 108000 / 111800 JSON files\n",
      "Processed 108000 / 111800 JSON files\n",
      "Processed 109000 / 111800 JSON files\n",
      "Processed 109000 / 111800 JSON files\n",
      "Processed 110000 / 111800 JSON files\n",
      "Processed 110000 / 111800 JSON files\n",
      "Processed 111000 / 111800 JSON files\n",
      "Processed 111000 / 111800 JSON files\n",
      "Total samples: 111800\n",
      "Total samples: 111800\n"
     ]
    }
   ],
   "source": [
    "print('starting to load YT dataset...')\n",
    "samples = load_yt_dataset_fast(YT_ROOTS)\n",
    "print(\"Total samples:\", len(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8fe4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "text_queries = [\n",
    "    \"vehicle\",\n",
    "    \"car tail light\",\n",
    "    \"brake light\",\n",
    "    \"left turn signal\",\n",
    "    \"right turn signal\",\n",
    "    \"car hazard lights\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529d03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(img_path, ann_objects, draw=False):\n",
    "    # Load image (still CPU, needed for plotting)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    width, height = img.size\n",
    "\n",
    "    # Prepare inputs and move to device\n",
    "    inputs = processor(text=text_queries, images=img, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Forward pass on GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post-process on CPU (Hugging Face expects CPU tensors here)\n",
    "    target_sizes = torch.tensor([(height, width)])\n",
    "    result = processor.post_process_grounded_object_detection(\n",
    "        outputs=outputs,\n",
    "        target_sizes=target_sizes,\n",
    "        threshold=0.1,\n",
    "        text_labels=[text_queries], \n",
    "    )[0]\n",
    "\n",
    "    boxes, scores, labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n",
    "\n",
    "    # Move tensors to GPU for loss computation\n",
    "    boxes = boxes.to(device)\n",
    "    scores = scores.to(device)\n",
    "\n",
    "    # Compute loss (on GPU)\n",
    "    loss_value = taillight_loss(\n",
    "        pred_boxes=boxes,\n",
    "        pred_labels=labels,\n",
    "        pred_scores=scores,\n",
    "        ann_objects=ann_objects,\n",
    "    )\n",
    "\n",
    "    if draw:\n",
    "        # Plot on CPU\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 7))\n",
    "        ax.imshow(img)\n",
    "\n",
    "        # Ground truth\n",
    "        for obj in ann_objects:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (x, y), w, h, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "                )\n",
    "            )\n",
    "            ax.text(x, y - 5, obj[\"label\"], color=\"red\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "        # Predictions\n",
    "        for box, score, label in zip(boxes.cpu(), scores.cpu(), labels):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (x1, y1),\n",
    "                    w,\n",
    "                    h,\n",
    "                    linewidth=1.2,\n",
    "                    edgecolor=\"lime\",\n",
    "                    facecolor=\"none\",\n",
    "                    zorder=10,\n",
    "                )\n",
    "            )\n",
    "            ax.text(\n",
    "                x1,\n",
    "                y1 - 5,\n",
    "                f\"{label} ({score:.2f})\",\n",
    "                color=\"lime\",\n",
    "                fontsize=11,\n",
    "                weight=\"bold\",\n",
    "                zorder=11,\n",
    "            )\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        print(\"Loss:\", loss_value.item())\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c9a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def taillight_loss(pred_boxes, pred_labels, pred_scores, ann_objects):\n",
    "    \"\"\"\n",
    "    Computes a unified loss for taillight state detection.\n",
    "    Lower = better.\n",
    "    \n",
    "    Components:\n",
    "        - IoU loss: (1 - IoU)\n",
    "        - Class loss: 0 if correct, weight if wrong\n",
    "        - Confidence loss: (1 - score)\n",
    "        - Brake mistakes weighted heavier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Map model label â†’ canonical GT label\n",
    "    label_map = {\n",
    "        \"vehicle\": \"off\",\n",
    "        \"car tail light\": \"off\",\n",
    "        \"brake light\": \"brake_on\",\n",
    "        \"left turn signal\": \"left_signal\",\n",
    "        \"right turn signal\": \"right_signal\",\n",
    "        \"car hazard lights\": \"hazard\",\n",
    "    }\n",
    "\n",
    "    # Classification penalty weights (mistakes in brake_on cost more)\n",
    "    class_penalty = {\n",
    "        \"brake_on\": 4.0,       # biggest penalty\n",
    "        \"left_signal\": 2.0,\n",
    "        \"right_signal\": 2.0,\n",
    "        \"hazard\": 2.5,\n",
    "        \"off\": 1.0,\n",
    "        \"brake_off\": 1.0,\n",
    "    }\n",
    "    if len(ann_objects) == 0:\n",
    "        return 10.0 * len(pred_boxes) / math.log(len(pred_boxes) + 2)  # no objects = no loss\n",
    "\n",
    "    if pred_boxes is None or len(pred_boxes) == 0:\n",
    "        # Penalize missing all predictions\n",
    "        return 10.0 * len(ann_objects) / math.log(len(ann_objects) + 1)\n",
    "\n",
    "    # Convert GT boxes\n",
    "    gt_boxes = []\n",
    "    gt_labels = []\n",
    "    for obj in ann_objects:\n",
    "        x, y, w, h = obj[\"bbox\"]\n",
    "        gt_boxes.append([x, y, x + w, y + h])\n",
    "        gt_labels.append(obj[\"label\"])\n",
    "\n",
    "    gt_boxes = torch.tensor(gt_boxes).float()\n",
    "\n",
    "    # IoU matrix\n",
    "    iou_mat = ops.box_iou(pred_boxes, gt_boxes)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # For each GT object, match the best prediction\n",
    "    for gi, gt_label in enumerate(gt_labels):\n",
    "        best_pi = torch.argmax(iou_mat[:, gi])\n",
    "        best_iou = iou_mat[best_pi, gi].item()\n",
    "\n",
    "        pred_label_raw = pred_labels[best_pi]\n",
    "        pred_label = label_map.get(pred_label_raw, \"off\")\n",
    "        pred_conf = pred_scores[best_pi].item()\n",
    "\n",
    "        # --- Loss components ---\n",
    "        iou_loss = 1 - best_iou                   # 0 when perfect overlap\n",
    "        conf_loss = 1 - pred_conf                # 0 when high confidence\n",
    "\n",
    "        if pred_label != gt_label:\n",
    "            class_loss = class_penalty.get(gt_label, 1.0)\n",
    "        else:\n",
    "            class_loss = 0.0\n",
    "\n",
    "        # Total loss per object\n",
    "        obj_loss = iou_loss + conf_loss + class_loss\n",
    "        total_loss += obj_loss\n",
    "        # if draw:\n",
    "            # print(total_loss / math.log(len(ann_objects) + 1))\n",
    "    return total_loss / math.log(len(ann_objects) + 1)  # average over GT objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237b5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     sample = samples[random.randint(0, len(samples) - 1)]\n",
    "#     print(f\"Showing sample {i}: {sample['image_path']}\")\n",
    "#     visualize_prediction(sample[\"image_path\"], sample[\"objects\"], draw=True)\n",
    "\n",
    "# loss = 0.0\n",
    "# for i in range(5):\n",
    "#     sample = samples[random.randint(0, len(samples) - 1)]\n",
    "#     loss += visualize_prediction(sample[\"image_path\"], sample[\"objects\"], draw=True)\n",
    "# print(loss / 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cb4f3",
   "metadata": {},
   "source": [
    "Where does OWL-ViT fail in this safety-critical scenario?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a89355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"brake_off\": \"car with brake light off\",\n",
    "    \"brake_on\": \"car with brake light on\",\n",
    "    \"left_signal\": \"car with left signal on\",\n",
    "    \"right_signal\": \"car with right signal on\",\n",
    "    \"hazard\": \"car with hazard lights on\",\n",
    "}\n",
    "\n",
    "text_queries = [label_map[key] for key in label_map.keys()]\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "label_to_idx = {key: idx for idx, key in enumerate(label_map.keys())}\n",
    "\n",
    "\n",
    "class OwlViTDataset(Dataset):\n",
    "    def __init__(self, records, label_map, min_boxes=1):\n",
    "        self.label_map = label_map\n",
    "        self.records = [\n",
    "            record\n",
    "            for record in records\n",
    "            if sum(obj[\"label\"] in label_map for obj in record[\"objects\"]) >= min_boxes\n",
    "        ]\n",
    "        self.label_to_idx = label_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.records[idx]\n",
    "        img_path = item[\"image_path\"]\n",
    "        objects = item[\"objects\"]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "\n",
    "        boxes = []\n",
    "        labels_idx = []\n",
    "        for obj in objects:\n",
    "            class_name = obj[\"label\"]\n",
    "            if class_name not in self.label_map:\n",
    "                continue\n",
    "\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            x1 = max(x / width, 0.0)\n",
    "            y1 = max(y / height, 0.0)\n",
    "            x2 = min((x + w) / width, 1.0)\n",
    "            y2 = min((y + h) / height, 1.0)\n",
    "\n",
    "            boxes.append([float(x1), float(y1), float(x2), float(y2)])\n",
    "            labels_idx.append(self.label_to_idx[class_name])\n",
    "\n",
    "        if boxes:\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "        else:\n",
    "            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if labels_idx:\n",
    "            labels_tensor = torch.tensor(labels_idx, dtype=torch.long)\n",
    "        else:\n",
    "            labels_tensor = torch.zeros((0,), dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"boxes\": boxes_tensor,\n",
    "            \"labels\": labels_tensor,\n",
    "            \"image_path\": img_path,\n",
    "            \"size\": (height, width),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1a98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(records, train_ratio=0.85, seed=42):\n",
    "    indices = list(range(len(records)))\n",
    "    random.Random(seed).shuffle(indices)\n",
    "    split_idx = max(1, int(len(indices) * train_ratio))\n",
    "    train_records = [records[i] for i in indices[:split_idx]]\n",
    "    val_records = [records[i] for i in indices[split_idx:]]\n",
    "    if not val_records:\n",
    "        val_records = train_records[-1:]\n",
    "        train_records = train_records[:-1]\n",
    "    return train_records, val_records\n",
    "\n",
    "\n",
    "def owlvit_collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    batch_text = [text_queries] * len(images)\n",
    "    encoded = processor(images=images, text=batch_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": encoded[\"pixel_values\"],\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "        \"gt_boxes\": [item[\"boxes\"] for item in batch],\n",
    "        \"gt_labels\": [item[\"labels\"] for item in batch],\n",
    "        \"image_paths\": [item[\"image_path\"] for item in batch],\n",
    "    }\n",
    "\n",
    "\n",
    "def cxcywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return torch.stack([x1, y1, x2, y2], dim=-1)\n",
    "\n",
    "\n",
    "def xyxy_to_cxcywh(boxes):\n",
    "    x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    cx = x1 + 0.5 * w\n",
    "    cy = y1 + 0.5 * h\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def owlvit_detection_loss(outputs, gt_boxes, gt_labels, cls_weight=1.0, box_weight=2.0, iou_weight=1.0):\n",
    "    pred_boxes = outputs.pred_boxes\n",
    "    pred_logits = outputs.logits\n",
    "    device = pred_boxes.device\n",
    "    total_loss = torch.zeros(1, device=device)\n",
    "    matched_batches = 0\n",
    "\n",
    "    for batch_idx in range(len(gt_boxes)):\n",
    "        boxes = gt_boxes[batch_idx].to(device)\n",
    "        labels = gt_labels[batch_idx].to(device)\n",
    "        if boxes.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        preds_xyxy = cxcywh_to_xyxy(pred_boxes[batch_idx])\n",
    "        ious = ops.box_iou(preds_xyxy, boxes)\n",
    "        best_idx = torch.argmax(ious, dim=0)\n",
    "        matched_logits = pred_logits[batch_idx][best_idx]\n",
    "        cls_loss = F.cross_entropy(matched_logits, labels)\n",
    "\n",
    "        pred_cxcywh = pred_boxes[batch_idx][best_idx]\n",
    "        target_cxcywh = xyxy_to_cxcywh(boxes)\n",
    "        box_loss = F.l1_loss(pred_cxcywh, target_cxcywh)\n",
    "\n",
    "        matched_ious = ious[best_idx, torch.arange(len(boxes), device=device)]\n",
    "        iou_loss = (1.0 - matched_ious.clamp(0.0, 1.0)).mean()\n",
    "\n",
    "        total_loss = total_loss + cls_weight * cls_loss + box_weight * box_loss + iou_weight * iou_loss\n",
    "        matched_batches += 1\n",
    "\n",
    "    if matched_batches == 0:\n",
    "        return total_loss\n",
    "\n",
    "    return total_loss / matched_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e862888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 54403 | Val samples: 23350\n"
     ]
    }
   ],
   "source": [
    "train_records, val_records = train_val_split(samples, train_ratio=0.70, seed=42)\n",
    "train_dataset = OwlViTDataset(train_records, label_map)\n",
    "val_dataset = OwlViTDataset(val_records, label_map)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bcbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 4\n",
    "learning_rate = 5e-6\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=owlvit_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=owlvit_collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "total_train_steps = max(1, len(train_loader) * num_epochs)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99ef98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b075c34f684e50a7738435a7c6d1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/3401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345a73892e014c07a783745e49440a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_epoch(data_loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    steps = 0\n",
    "    loop = tqdm(data_loader, desc=\"train\" if train else \"val\", leave=False)\n",
    "\n",
    "    for batch in loop:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                pixel_values=pixel_values,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            loss = owlvit_detection_loss(\n",
    "                outputs,\n",
    "                gt_boxes=batch[\"gt_boxes\"],\n",
    "                gt_labels=batch[\"gt_labels\"],\n",
    "            )\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        steps += 1\n",
    "        loop.set_postfix(loss=epoch_loss / max(1, steps))\n",
    "\n",
    "    return epoch_loss / max(1, steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
